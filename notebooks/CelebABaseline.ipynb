{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gDcFUVaFf-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import pandas as pd\n",
        "import importlib\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SDlFgKYiZWp",
        "colab_type": "code",
        "outputId": "d6a6ff1c-29c2-4ee1-a0ce-009636ee89b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "print(\"Mounted. Copying CelebA...\")\n",
        "!cp -r /content/gdrive/My\\ Drive/celeba/ /content/celeba/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted. Copying CelebA...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD4INwMFKKhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(is_training:bool):\n",
        "  if is_training:\n",
        "    t = transforms.Compose(\n",
        "          [\n",
        "          transforms.RandomGrayscale(),\n",
        "          transforms.RandomApply([transforms.RandomRotation(45)], p=0.1),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomVerticalFlip(),\n",
        "          transforms.ToTensor()\n",
        "          ]\n",
        "        )\n",
        "  else:\n",
        "    t = transforms.ToTensor()\n",
        "  dataset = torchvision.datasets.celeba.CelebA(\"/content\", download=True, transform=t, split=\"train\" if is_training else \"test\")\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ4bZOZiVK28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loader(is_training:bool, batch_size:int):\n",
        "  dataset = get_dataset(is_training)\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=is_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NESrwMH_LlL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureClassifier(nn.Module):\n",
        "  def __init__(self, n_inp, n_out):\n",
        "    super(FeatureClassifier, self).__init__()\n",
        "    midpoint = n_inp + ( n_out - n_inp) // 2\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(n_inp, midpoint),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(midpoint, n_out)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmntO3ZvLCQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestModel(nn.Module):\n",
        "  def __init__(self, single_output=False):\n",
        "    super(TestModel, self).__init__()\n",
        "\n",
        "    self.resnet = torchvision.models.resnet50()\n",
        "    \n",
        "    if single_output:\n",
        "      self.decoder = FeatureClassifier(1000, 40)\n",
        "    else:\n",
        "      self.decoders = [ FeatureClassifier(1000, 2) for _ in range(40)]\n",
        "      self.decoder = lambda x: [ d(x) for d in self.decoders ]\n",
        "      for i, d in enumerate(self.decoders):\n",
        "        self.add_module(f\"Decoder{i}\", d)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    features = self.resnet(x)\n",
        "    decoded = self.decoder(features)\n",
        "    return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QkjUXCy16Zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, valid_loader, num_epochs:int, single_output:bool, valid_freq:int=10):\n",
        "  \"\"\"\n",
        "    optimizer: Either a single optimizer if single_output = True, or a list of optimizers. The first optimizer is for the resnet and the rest\n",
        "               are for each classifier, for a total of 41. \n",
        "  \"\"\"\n",
        "  train_len = len(train_loader)\n",
        "  valid_len = len(valid_loader)\n",
        "  loop = tqdm(total=(num_epochs * train_len + (num_epochs // valid_freq) * valid_len), position=0)\n",
        "\n",
        "  train_losses = []\n",
        "  train_accs = []\n",
        "\n",
        "  valid_losses = []\n",
        "  valid_accs = []\n",
        "\n",
        "  criteria = nn.BCEWithLogitsLoss() if single_output else nn.CrossEntropyLoss()\n",
        "\n",
        "  for e in range(num_epochs):\n",
        "    loss_builder = []\n",
        "    acc_builder = []\n",
        "\n",
        "    for i, (x, y_truth) in enumerate(train_loader):\n",
        "      x, y_truth = x.cuda(async=False), y_truth.cuda(async=False)\n",
        "\n",
        "      if single_output:\n",
        "        y_truth = y_truth.float()\n",
        "        optimizer.zero_grad()\n",
        "      else:\n",
        "        y_truth = y_truth.long()\n",
        "        [o.zero_grad() for o in optimizer]\n",
        "      \n",
        "      y_hat = model(x)\n",
        "\n",
        "      if single_output:\n",
        "        # import pdb; pdb.set_trace()\n",
        "        total_loss = criteria(y_hat, y_truth)\n",
        "        total_loss.backward()\n",
        "        acc = (y_truth.eq(y_hat >= 0.5).sum(dim=1) == 40).sum() / len(y_truth)\n",
        "        optimizer.step()\n",
        "\n",
        "      else:\n",
        "        # import pdb; pdb.set_trace()\n",
        "        # y_hat is an array of tensors, with total shape of F x B x 2\n",
        "        # y_truth is a tensor of shape of B x F\n",
        "        # we need to iterate over the F dimension.\n",
        "        losses = []\n",
        "        for i in range(y_truth.shape[1]):\n",
        "          loss = criteria(y_hat[i], y_truth[:,i])\n",
        "          losses.append(loss)\n",
        "        # for l, o in zip(losses, optimizer[1:]):\n",
        "          # l.backward(retain_graph=True)\n",
        "          # o.step()\n",
        "        \n",
        "        # Update the ResNet\n",
        "        total_loss = sum(losses)\n",
        "        total_loss.backward()\n",
        "        optimizer[0].step()\n",
        "        \n",
        "        # import pdb; pdb.set_trace()\n",
        "        acc = (y_truth.eq(torch.argmax(torch.stack(y_hat, dim=0), dim=2).permute(1, 0)).sum(dim=1) == 40).sum() / y_truth.shape[0]\n",
        "\n",
        "      loss_builder.append(total_loss.item())\n",
        "      acc_builder.append(acc)\n",
        "\n",
        "      loop.update(1)\n",
        "      loop.set_description(f\"Epoch: {e}, it: {i}/{train_len}. Loss: {total_loss.item()}. Acc: {acc}\")\n",
        "    \n",
        "    train_accs.append(acc_builder)\n",
        "    train_losses.append(loss_builder)\n",
        "\n",
        "    if e % valid_freq == 0:\n",
        "      loss_builder = []\n",
        "      acc_builder = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for i, (x, y_truth) in enumerate(train_loader):\n",
        "          x, y_truth = x.cuda(async=False), y_truth.cuda(async=False)\n",
        "\n",
        "        if single_output:\n",
        "          y_truth = y_truth.float()\n",
        "        else:\n",
        "          y_truth = y_truth.long()\n",
        "          \n",
        "          y_hat = model(x)\n",
        "\n",
        "          if single_output:\n",
        "            total_loss = criteria(y_hat, y_truth)\n",
        "            acc = (y_truth.eq(y_hat >= 0.5).sum(dim=1) == 40).sum() / len(y_truth)\n",
        "          else:\n",
        "            # y_hat is an array of tensors, with total shape of F x B x 2\n",
        "            # y_truth is a tensor of shape of B x F\n",
        "            # we need to iterate over the F dimension.\n",
        "            losses = []\n",
        "            for i in range(y_truth.shape[1]):\n",
        "              loss = criteria(y_hat[i], y_truth[:,i])\n",
        "              losses.append(loss)\n",
        "\n",
        "            # Update the ResNet\n",
        "            total_loss = sum(losses)\n",
        "\n",
        "            acc = (y_truth.eq(torch.argmax(torch.stack(y_hat, dim=0), dim=2).permute(1, 0)).sum(dim=1) == 40).sum() / y_truth.shape[0]\n",
        "\n",
        "            loss_builder.append(total_loss)\n",
        "            acc_builder.append(acc)\n",
        "\n",
        "            loop.update(1)\n",
        "            loop.set_description(f\"[VALIDATING] Epoch: {e}, it: {i}/{valid_len}. Loss: {total_loss.item()}. Acc: {acc}\")\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      valid_accs.append(acc_builder)\n",
        "      valid_losses.append(loss_builder)\n",
        "    \n",
        "    state = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_accs\": train_accs,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"valid_accs\": valid_accs,\n",
        "        \"epoch\": e\n",
        "    }\n",
        "    num = ((e + 4) // 5) * 5\n",
        "    path = f\"/content/gdrive/My Drive/SimCLR/models/CelebA/baseline_single_{single_output}_e_{num}.mod\"\n",
        "    torch.save(state, path)\n",
        "\n",
        "  return train_losses, train_accs, valid_losses, valid_accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ULtSDcSr8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training(single_output=False):\n",
        "  batch_size = 64 if single_output else 64\n",
        "  model = TestModel(single_output=single_output).cuda()\n",
        "  if single_output:\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "  else:\n",
        "    optimizer = [ optim.Adam(model.parameters()) ]\n",
        "    # optimizer = [ optim.Adam(model.resnet.parameters()) ]\n",
        "    # optimizer += [ optim.Adam(d.parameters()) for d in model.decoders ]\n",
        "\n",
        "  train_loader = get_loader(True, batch_size)\n",
        "  valid_loader = get_loader(False, batch_size)\n",
        "\n",
        "  train_losses, train_accs, valid_losses, valid_accs = train(model, optimizer, train_loader, valid_loader, num_epochs=20, single_output=single_output, valid_freq=10)\n",
        "  return train_losses, train_accs, valid_losses, valid_accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQO6U2jzUG_a",
        "colab_type": "code",
        "outputId": "97d25896-921a-41c4-da07-e589c178e7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "run_training(True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /content/celeba/img_align_celeba.zip\n",
            "Using downloaded and verified file: /content/celeba/list_attr_celeba.txt\n",
            "Using downloaded and verified file: /content/celeba/identity_CelebA.txt\n",
            "Using downloaded and verified file: /content/celeba/list_bbox_celeba.txt\n",
            "Using downloaded and verified file: /content/celeba/list_landmarks_align_celeba.txt\n",
            "Using downloaded and verified file: /content/celeba/list_eval_partition.txt\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2, it: 2198/2544. Loss: 0.23239925503730774. Acc: 0:  14%|█▍        | 7287/51504 [1:23:36<8:09:01,  1.51it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Wj5INGA4DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}