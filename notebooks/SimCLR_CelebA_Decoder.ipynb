{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimCLR_CelebA_Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg8E_b23lkP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import pandas as pd\n",
        "import importlib\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ6TXy33R6w0",
        "colab_type": "code",
        "outputId": "bb333e8c-f2ef-487c-ffde-d495a691f414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW4TA2f-U51N",
        "colab_type": "code",
        "outputId": "6860f0c4-1a87-437b-db4f-d538a1c3ec5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!rm -rf CS601R_Final_Project/ models/ datasets/ notebooks/\n",
        "!git clone https://github.com/webMan1/CS601R_Final_Project.git\n",
        "!cp -r CS601R_Final_Project/* ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CS601R_Final_Project'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 25 (delta 5), reused 24 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4hStpoTVP6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import models.ResNetDecoder\n",
        "import importlib\n",
        "\n",
        "models.ResNetDecoder = importlib.reload(models.ResNetDecoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geLY-EOZSCCn",
        "colab_type": "text"
      },
      "source": [
        "# Define Datasets and Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOAJkoGVSAYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class EncodedCelebADataset(Dataset):\n",
        "    '''\n",
        "    one of target_col_idx or target should be supplied:\n",
        "        target_col_idx specifies the column index from the attributes to use as the label\n",
        "        target specifies a target that spans multiple columns\n",
        "    '''\n",
        "    def __init__(self, data_root, split='train', target_col_idx=None, target=None):\n",
        "        split_map = {\n",
        "            \"train\": 0,\n",
        "            \"valid\": 1,\n",
        "        }\n",
        "\n",
        "        assert split in split_map\n",
        "        assert (target_col_idx or target) and not (target_col_idx and target)\n",
        "        if target:\n",
        "            assert target in ['hair']\n",
        "\n",
        "        splits = pd.read_csv(os.path.join(data_root, \"list_eval_partition.txt\"), delim_whitespace=True, header=None, index_col=0)\n",
        "        attr = pd.read_csv(os.path.join(data_root, \"list_attr_celeba.txt\"), delim_whitespace=True, header=1)\n",
        "        mask = splits[1] == split_map[split]\n",
        "\n",
        "        if target_col_idx:\n",
        "            print(f'Using the {attr.columns[target_col_idx]} attribute as the label')\n",
        "            self.labels = torch.as_tensor(attr[mask].values[:, target_col_idx])\n",
        "            # self.labels = self.labels.reshape(-1, 1)\n",
        "        else:\n",
        "            # TODO: probably should do some argmax or something\n",
        "            print(f'Using a group of columns to represent {target}')\n",
        "            if target == 'hair':\n",
        "                # bald, black_hair, blonde_hair, brown_hair, gray_hair, receding_hairline\n",
        "                col_idxs = [4, 8, 9, 11, 17, 28]\n",
        "            self.labels = torch.as_tensor(attr[mask].values[:, col_idxs])\n",
        "        self.labels = (self.labels + 1) // 2 # changes it from -1,1 to 0,1\n",
        "        print(self.labels.shape)\n",
        "        print(f\"Mask shape: {mask.shape}\")\n",
        "        self.encodings = torch.load(os.path.join(data_root, f'{split}_encodings.pt'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      try:\n",
        "        return self.encodings[i], self.labels[i]\n",
        "      except:\n",
        "        import pdb; pdb.set_trace()\n",
        "        raise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSCxvVCNTpC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loader(is_training:bool, batch_size:int):\n",
        "  target_cols = list(range(40)) # I'm pretty sure there should be 40 columns..,\n",
        "  dataset =  EncodedCelebADataset(\"/content/gdrive/My Drive/SimCLR/data/celeba\", split=\"train\" if is_training else \"valid\", target_col_idx=target_cols)\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=is_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHhv7VVNVHKD",
        "colab_type": "text"
      },
      "source": [
        "# Define the decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOx7hTrUUs_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CLRDecoder(nn.Module):\n",
        "  def __init__(self, single_output=True):\n",
        "    super(CLRDecoder, self).__init__()\n",
        "    self.adapter = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(2048, 1000),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.decoder = models.ResNetDecoder.ResNetDecoder(single_output=single_output)\n",
        "    self.net = nn.Sequential(self.adapter, self.decoder)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MJ2LgwKX5hV",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUOFnkxGX47o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, valid_loader, num_epochs:int, single_output:bool, valid_freq:int=10):\n",
        "  train_len = len(train_loader)\n",
        "  valid_len = len(valid_loader)\n",
        "  loop = tqdm(total=(num_epochs * train_len + (num_epochs // valid_freq) * valid_len), position=0)\n",
        "\n",
        "  train_losses = []\n",
        "  train_accs = []\n",
        "  train_faccs = []\n",
        "\n",
        "  valid_losses = []\n",
        "  valid_accs = []\n",
        "  valid_faccs = []\n",
        "\n",
        "  criteria = nn.BCEWithLogitsLoss() if single_output else nn.CrossEntropyLoss()\n",
        "\n",
        "  sigmoid = nn.Sigmoid()\n",
        "\n",
        "  for e in range(num_epochs):\n",
        "    loss_builder = []\n",
        "    acc_builder = []\n",
        "    facc_builder = []\n",
        "\n",
        "    for i, (x, y_truth) in enumerate(train_loader):\n",
        "      x, y_truth = x.cuda(async=False), y_truth.cuda(async=False)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      if single_output:\n",
        "        y_truth = y_truth.float()\n",
        "\n",
        "      else:\n",
        "        y_truth = y_truth.long()\n",
        "      \n",
        "      y_hat = model(x)\n",
        "\n",
        "      if single_output:\n",
        "        # import pdb; pdb.set_trace()\n",
        "        total_loss = criteria(y_hat, y_truth)\n",
        "        total_loss.backward()\n",
        "        y_hat_probs = sigmoid(y_hat)\n",
        "        acc = (y_truth.eq(y_hat_probs >= 0.5).sum(dim=1) == 40).sum().item() / len(y_truth)\n",
        "        facc = y_truth.eq(y_hat_probs >= 0.5).sum().item() / y_truth.numel()\n",
        "        optimizer.step()\n",
        "\n",
        "      else:\n",
        "        # import pdb; pdb.set_trace()\n",
        "        # y_hat is an array of tensors, with total shape of F x B x 2\n",
        "        # y_truth is a tensor of shape of B x F\n",
        "        # we need to iterate over the F dimension.\n",
        "        losses = []\n",
        "        for i in range(y_truth.shape[1]):\n",
        "          loss = criteria(y_hat[i], y_truth[:,i])\n",
        "          losses.append(loss)\n",
        "        # for l, o in zip(losses, optimizer[1:]):\n",
        "          # l.backward(retain_graph=True)\n",
        "          # o.step()\n",
        "        \n",
        "        # Update the ResNet\n",
        "        total_loss = sum(losses)\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # import pdb; pdb.set_trace()\n",
        "        acc = (y_truth.eq(torch.argmax(torch.stack(y_hat, dim=0), dim=2).permute(1, 0)).sum(dim=1) == 40).sum().item() / y_truth.shape[0]\n",
        "        facc = y_turth.eq(torch.argmax(torch.stack(y_hat, dim=0), dim=2).permute(1, 0)).sum().item() / y_truth.numel()\n",
        "\n",
        "      loss_builder.append(total_loss.item())\n",
        "      acc_builder.append(acc)\n",
        "      facc_builder.append(facc)\n",
        "\n",
        "      loop.update(1)\n",
        "      loop.set_description(f\"Epoch: {e}, it: {i}/{train_len}. Loss: {total_loss.item()}. Acc: {acc}\")\n",
        "    \n",
        "    train_accs.append(acc_builder)\n",
        "    train_losses.append(loss_builder)\n",
        "    train_faccs.append(facc_builder)\n",
        "\n",
        "    if e % valid_freq == 0:\n",
        "      loss_builder = []\n",
        "      acc_builder = []\n",
        "      facc_builder = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for i, (x, y_truth) in enumerate(train_loader):\n",
        "          x, y_truth = x.cuda(async=False), y_truth.cuda(async=False)\n",
        "\n",
        "        if single_output:\n",
        "          y_truth = y_truth.float()\n",
        "        else:\n",
        "          y_truth = y_truth.long()\n",
        "          \n",
        "        y_hat = model(x)\n",
        "\n",
        "        if single_output:\n",
        "          total_loss = criteria(y_hat, y_truth)\n",
        "          y_hat_probs = sigmoid(y_hat)\n",
        "          acc = (y_truth.eq(y_hat_probs >= 0.5).sum(dim=1) == 40).sum().item() / len(y_truth)\n",
        "          facc = y_truth.eq(y_hat_probs >= 0.5).sum().item() / y_truth.numel()\n",
        "        else:\n",
        "          # y_hat is an array of tensors, with total shape of F x B x 2\n",
        "          # y_truth is a tensor of shape of B x F\n",
        "          # we need to iterate over the F dimension.\n",
        "          losses = []\n",
        "          for i in range(y_truth.shape[1]):\n",
        "            loss = criteria(y_hat[i], y_truth[:,i])\n",
        "            losses.append(loss)\n",
        "\n",
        "          # Update the ResNet\n",
        "          total_loss = sum(losses)\n",
        "\n",
        "          acc = (y_truth.eq(torch.argmax(torch.stack(y_hat, dim=0), dim=2).permute(1, 0)).sum(dim=1) == 40).sum().item() / y_truth.shape[0]\n",
        "          facc = y_turth.eq(torch.argmax(torch.stack(y_hat, dim=0), dim=2).permute(1, 0)).sum().item() / y_truth.numel()\n",
        "\n",
        "        loss_builder.append(total_loss.item())\n",
        "        acc_builder.append(acc)\n",
        "\n",
        "        loop.update(1)\n",
        "        loop.set_description(f\"[VALIDATING] Epoch: {e}, it: {i}/{valid_len}. Loss: {total_loss.item()}. Acc: {acc}\")\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      valid_accs.append(acc_builder)\n",
        "      valid_losses.append(loss_builder)\n",
        "      valid_faccs.append(facc_builder)\n",
        "    \n",
        "    state = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_accs\": train_accs,\n",
        "        \"train_faccs\": train_faccs,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"valid_accs\": valid_accs,\n",
        "        \"valid_faccs\": valid_faccs,\n",
        "        \"epoch\": e\n",
        "    }\n",
        "    num = ((e + 4) // 5) * 5\n",
        "    path = f\"/content/gdrive/My Drive/SimCLR/models/CelebA/clr_single_{single_output}_e_{num}.mod\"\n",
        "    torch.save(state, path)\n",
        "\n",
        "  return train_losses, train_accs, valid_losses, valid_accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCs0rNj3YDZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training(single_output=False):\n",
        "  batch_size = 64 if single_output else 64\n",
        "  model = CLRDecoder(single_output=single_output).cuda()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  train_loader = get_loader(True, batch_size)\n",
        "  valid_loader = get_loader(False, batch_size)\n",
        "\n",
        "  train_losses, train_accs, valid_losses, valid_accs = train(model, optimizer, train_loader, valid_loader, num_epochs=71, single_output=single_output, valid_freq=10)\n",
        "\n",
        "  return train_losses, train_accs, valid_losses, valid_accs, valid_faccs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voqBJGrkYbEX",
        "colab_type": "code",
        "outputId": "2a15b4d0-3fe2-4555-9050-6dc6fd830ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        }
      },
      "source": [
        "res = run_training(True)\n",
        "rest2 = run_training(False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the Index(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
            "       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
            "       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
            "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
            "       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
            "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
            "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n",
            "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
            "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'],\n",
            "      dtype='object') attribute as the label\n",
            "torch.Size([162770, 40])\n",
            "Mask shape: (202599,)\n",
            "Using the Index(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
            "       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
            "       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
            "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
            "       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
            "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
            "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n",
            "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
            "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'],\n",
            "      dtype='object') attribute as the label\n",
            "torch.Size([19867, 40])\n",
            "Mask shape: (202599,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[VALIDATING] Epoch: 70, it: 2543/311. Loss: 0.18691235780715942. Acc: 0.0:  99%|█████████▉| 180632/182801 [18:59<00:13, 160.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5b1d3b979cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ab2ab58bba9d>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(single_output)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m71\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_faccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_faccs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_faccs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrNh5oIDqjEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses, train_accs, valid_losses, valid_accs = res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QzHtqIsqmbZ",
        "colab_type": "code",
        "outputId": "4a80610d-3983-439d-dee7-b89c479f6e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "valid_accs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.05555555555555555]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-t7ZmX0qMj_",
        "colab_type": "text"
      },
      "source": [
        "# Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYR7NBUJo6Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_res(single:bool, epoch:int):\n",
        "  save_loc = f\"/content/gdrive/My Drive/SimCLR/models/CelebA/clr_single_{single}_e_{epoch}.mod\"\n",
        "  state = torch.load(save_loc)\n",
        "  \n",
        "  epoch = state[\"epoch\"]\n",
        "  \n",
        "  train_losses, train_accs = state[\"train_losses\"], state[\"train_accs\"]\n",
        "  valid_losses, valid_accs = state[\"valid_losses\"], state[\"valid_accs\"]\n",
        "\n",
        "  train_loss_averages = [ np.mean(l) for l in train_losses]\n",
        "  valid_loss_averages = [ np.mean([i for i in l]) for l in valid_losses]\n",
        "\n",
        "  train_acc_averages = [ np.mean([i for i in l]) for l in train_accs]\n",
        "  valid_acc_averages = [ np.mean([i for i in l]) for l in valid_accs]\n",
        "\n",
        "  print(f\"Got to epoch {epoch}\")\n",
        "  plt.plot(train_loss_averages)\n",
        "  plt.plot(range(0, len(train_loss_averages), 10), valid_loss_averages)\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(train_acc_averages)\n",
        "  plt.plot(range(0, len(train_acc_averages), 10), valid_acc_averages)\n",
        "  plt.show()\n",
        "\n",
        "  return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_hZr2xUpF8s",
        "colab_type": "code",
        "outputId": "9be72e09-7bae-409c-ab27-f8646e3a0e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "state = show_res(True, 15)\n",
        "# state2 = show_res(False, 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-98ef09a03cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# state2 = show_res(False, 15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-a15e0825cfde>\u001b[0m in \u001b[0;36mshow_res\u001b[0;34m(single, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mvalid_loss_averages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mtrain_acc_averages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mvalid_acc_averages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_accs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-a15e0825cfde>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mvalid_loss_averages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mtrain_acc_averages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mvalid_acc_averages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_accs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3335\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'type'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtZ2FHvrphdt",
        "colab_type": "code",
        "outputId": "3eb94ee3-3f79-45e0-fbaa-2f22f4234441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "loader = get_loader(False, 32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the Index(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
            "       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
            "       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
            "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
            "       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
            "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
            "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n",
            "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
            "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'],\n",
            "      dtype='object') attribute as the label\n",
            "torch.Size([19867, 40])\n",
            "Mask shape: (202599,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfZFobWspIsj",
        "colab_type": "code",
        "outputId": "fb40a6a0-0f5f-424e-b163-36a90d42f141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "model = CLRDecoder(True)\n",
        "model.load_state_dict(state['model'])\n",
        "\n",
        "test_its = 100000000\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "accs = []\n",
        "naive_accs = []\n",
        "for i, (x, y) in zip(range(test_its), loader):\n",
        "  # pred = torch.stack(model(x)).permute(1, 0, 2).argmax(dim=2)\n",
        "  pred = sigmoid(model(x))\n",
        "  # print(\"Predicted:\")\n",
        "  # print((pred > 0.5).int().tolist())\n",
        "  # print(\"Actual\")\n",
        "  # print(y.tolist())\n",
        "\n",
        "  diff = (pred > 0.5).int().eq(y)\n",
        "  acc = diff.sum().item() / diff.numel()\n",
        "  accs.append(acc)\n",
        "\n",
        "  num_tg_ones = y.sum().item()\n",
        "  naive_acc = 1 - (num_tg_ones / diff.numel())\n",
        "  naive_accs.append(naive_acc)\n",
        "print(f\"Accuracy: {np.average(accs)}\")\n",
        "print(f\"Naive acc would be {np.average(naive_acc)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8934163833720999\n",
            "Naive acc would be 0.7833333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}